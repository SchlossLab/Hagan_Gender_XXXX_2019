---
title: "Supplementary Text"
bibliography: references.bib
csl: mbio.csl
output:
  pdf_document:
    includes:
      in_header: header.tex
    keep_tex: yes
  html_document:
    df_print: paged
geometry: margin=1.0in
fontsize: 11pt
always_allow_html: yes
---

```{r supp_setup, include=FALSE}
library(knitr)
library(kableExtra)
library(markdown)
library(rmarkdown)
library(formattable)
library(png)

source("../code/genderize/genderize_analysis.R")
source("../code/get_plot_options.R")

knitr::opts_chunk$set(echo = FALSE, warning = FALSE)
options(kableExtra.latex.load_packages = FALSE)
options(kableExtra.auto_format = FALSE)
options(knitr.table.format = "latex")
```



```{r genderize_setup, include=FALSE}
source("../code/genderize/b_c_comparison.R")
```

**Validation of gender inferrence.** We first validated the genderize.io algorithm using a set of `r total_names_ascii` names whose gender had been hand-coded based on appearance [@broderick_gender_2019]. The names were supplied to the genderize algorithm both with and without the accompanying country data. The genderize algorithm returned gender inferences for `r compared_names_ascii` queries when first names were given and `r country_ascii_compared_names` when country data was also supplied (`r compared_names_ascii - country_ascii_compared_names` names were associated with countries unsupported by genderize).
  
  Sensitivity and specificity are measurements of an algorithm's tendency to return correct answers instead of false positives (e.g., a man incorrectly gendered as a woman) or false negatives (e.g., a woman incorrectly gendered as a man). The closer these values are to 1, the smaller the chance that the algorithm will return the correlating false response. Accuracy is a composite measure of the algorithm's ability to differentiate the genders correctly. These measurements were calculated from the data sets (with and without country data supplied) at three different probability threshold cutoffs: the default genderize (0.5), a probability threshold of 0.85 (0.85), and a modified probability  of 0.85, which factors in the number of instances returned (pmod0.85; pmod = probability x count + 2/(count + 4))[@edwards_gender_2018;@holman_gender_2018]. 
  
   At the 0.5 threshold, the data set returned a sensitivity of `r b_c_ascii_summary[1,2]` and specificity of `r  b_c_ascii_summary[2,2]` for an accuracy of `r b_c_ascii_summary[3,2]`, compared to a marginally higher accuracy of `r b_c_country_ascii_summary[3,2]` for the data set where country data were included (Table S1). Generally speaking, the accuracy increases as the threshold increases, with slight trade offs between sensitivity and specificity. For the purposes of our analysis, we opted to use the pmod0.85 threshold for our analysis (Table S1, in bold). 

```{r country_impact, message=FALSE}
source("../code/genderize/b_c_country_na.R")
```
**Impact of geographic bias**  To understand the extent of geographic bias in our gender assignment against regions and languages with gender-less naming conventions or that lack social media for incorporation into the genderize algorithm, we compared the number of names inferred without associated country data to when country data was also supplied. In our test data set, the top five countries associated with names were the United States, Germany, the United Kingdom, France, and China. The countries with the highest proportion of un-inferred genders when country data were supplied are Cambodia, Iceland, Indonesia, Ireland, and Mexico, where the maximum number of names supplied ranged from `r min_names_five_na` to `r max_names_five_na`. To determine the impact of each country towards the overall percentage of names whose genders were not inferred (`r percent_unpredicted`%), we found the difference between the percent of names un-inferred for each country and the overall percentage, multiplied by the proportion of observations from that country to the total observations and finally divided by the overall percentage of un-inferred names (Fig. S8A). The top five countries with the greatest impact on un-inferred names, and thus the countries receiving the most negative bias from genderize were Canada, China, Ireland, Belgium, and Sweden. These data suggest that there is likely some bias against countries with gender-neutral naming conventions (e.g., China), and indicates the stringency with which the algorithm applies gender to names that are accompanied by country data. For instance, strongly gendered names such as Peter and Pedro were not assigned gender when associated with Canada.

```{r editor_genderize, message=FALSE}
source("../code/genderize/editor_comparison.R")
```
  We next applied the genderize algorithm at the pmod0.85 threshold to our journals data set and tested its validity on a small portion. All first names collected from our data set were submitted to genderize both with and without country data and only those with a pmod equivalent to or greater than 0.85 were retained. Next, the inferred genders were assigned to individuals as described previously (Fig. S7). Given the relatively small number of editors and senior editors in our data set, the presenting gender (man/woman) of editors and senior editors in our data set was hand-validated using Google where possible. Of the `r total_ed_names` editor names, `r compared_ed_names` were inferred by our application of genderize for an accuracy of `r round(ed_accuracy, digits = 4)`, thus increasing our confidence in the gender inferences. 

```{r ASM_genderize, message=FALSE}
source("../code/genderize/ASM_country_na.R")
```
  In our full data set, the five countries with the most individuals were the United States, China, Japan, France, and Germany. The countries with the highest proportion of un-inferred genders were Burundi, Chad, Kingman Reef, North Korea, and Maldives, where the maximum number of names supplied ranged from `r ASM_min_names_five_na` to `r ASM_max_names_five_na`. Proportionally, fewer names in our full data set were assigned gender than in our validation data set (`r ASM_percent_unpredicted`% un-inferred versus `r percent_unpredicted`% un-inferred, respectively). Since adjusting the workflow to infer the gender of names both with and without country data, the countries receiving the most negative bias from genderize were China, Japan, South Korea, India, and Taiwan (Fig. S8B). These data indicate what we previously inferred, that the genderize algorithm has bias against countries with gender-neutral naming conventions.

\vspace{40mm}

Table S1. Sensitivity, specificity, and accuracy of genderize thresholds for data provided by Broderick and Casadevall [@broderick_gender_2019]. Bolded text denotes the accuracy of the threshold used in all further analyses. Also included are the percent of names not assigned a gender by the API (percent unknown) at each threshold.
```{r genderize_stats, results='asis', echo=FALSE, message=FALSE}  
source("../code/genderize/Table_S1.R")
supp_table_1 <- stats_summary %>% 
  mutate(b_c_data_ascii_pmod85 = ifelse(b_c_data_ascii_pmod85 == "0.9714", 
                                        cell_spec(b_c_data_ascii_pmod85, bold = T),
                                        cell_spec(b_c_data_ascii_pmod85, background = "white")),
         b_c_country_data_ascii_pmod85 = ifelse(b_c_country_data_ascii_pmod85 == "0.9695", 
                                                cell_spec(b_c_country_data_ascii_pmod85, bold = T),
                                                cell_spec(b_c_country_data_ascii_pmod85, background = "white"))
  ) %>% 
  knitr::kable(., format = "latex", #table.attr = "style = \"color: black;\"", 
               digits = 4, col.names = c("Measure", "p0.5", "p0.85", "pmod0.85", "p0.5", "p0.85", "pmod0.85"), escape = F) %>%
  kable_styling() %>% 
  add_header_above(c(" " = 1, "First Names" = 3, "Plus Country Data" = 3)) %>% 
  footnote(general = "Bolded text denotes the accuracy of the threshold used in all further analyses") 

supp_table_1
```

